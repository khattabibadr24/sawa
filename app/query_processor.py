import requests
import json
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer

class QueryProcessor:
    def __init__(self, qdrant_host='localhost', qdrant_port=6333, collection_name='my_collection', score_threshold=0.3):
        self.qdrant_client = QdrantClient(
            host=qdrant_host,
            port=qdrant_port,
            check_compatibility=False
        )
        self.collection_name = collection_name
        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        self.score_threshold = score_threshold
        self.mistral_api_key = "a4urlm2FhhZLdxi7nz3iYTU0E80vHg22"
        self.mistral_model = "mistral-small"
        self.api_url = "https://api.mistral.ai/v1/chat/completions"

        self.prompts_presets = {
            "standard": "R√©ponds uniquement en fran√ßais. En utilisant uniquement le contexte ci-dessous, r√©ponds de fa√ßon claire √† la question suivante.",
            "bullet_points": "R√©ponds uniquement en fran√ßais et uniquement avec des bullet points clairs et synth√©tiques.",
            "friendly": "R√©ponds uniquement en fran√ßais. Sois chaleureux et professionnel. Utilise uniquement les informations du contexte.",
            "understand_first": "R√©ponds uniquement en fran√ßais. Avant de r√©pondre, assure-toi d'avoir bien compris le sens de la question. Utilise uniquement le contexte fourni pour formuler une r√©ponse claire et pertinente.",
            "greeting": "Bonjour ! Comment puis-je vous aider aujourd'hui ? "
        }

        self.chat_history = []  

    def is_greeting(self, query):
        """D√©tecte si la requ√™te est une salutation"""
        greetings = ["bonjour", "salut", "bonsoir", "hello", "coucou", "hi", "hey"]
        query_lower = query.strip().lower()
        return any(query_lower.startswith(word) for word in greetings)

    def generate_query_embedding(self, query):
        """G√©n√®re l'embedding pour une requ√™te"""
        return self.embedding_model.encode(query).tolist()

    def search_qdrant(self, query_embedding, top_k=3):
        """Recherche dans Qdrant avec l'embedding"""
        results = self.qdrant_client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            limit=top_k
        )
        return results

    def build_messages_with_history(self, user_query, context=None):
        """Construit les messages pour l'API Mistral en incluant l'historique"""
        messages = [{"role": "system", "content": "Tu es un assistant expert qui r√©pond de mani√®re claire, concise et utile."}]
        
        # Ajouter l'historique des conversations pr√©c√©dentes
        for msg in self.chat_history:
            messages.append({"role": "user", "content": msg["user"]})
            messages.append({"role": "assistant", "content": msg["assistant"]})
        
        # Ajouter la nouvelle question avec le contexte si disponible
        if context:
            user_content = f"(Contexte : {context})\n\n{user_query}"
        else:
            user_content = user_query
            
        messages.append({"role": "user", "content": user_content})
        
        return messages

    def stream_response(self, user_query, prompt_key="standard"):
        """
        M√©thode principale pour traiter une requ√™te avec streaming.
        G√®re les salutations, la recherche Qdrant, et la continuit√© de conversation.
        """
        print(f"[DEBUG] Requ√™te utilisateur : {user_query}")
        
        # 1. G√©rer les salutations
        if self.is_greeting(user_query):
            polite_response = self.prompts_presets.get("greeting", "Bonjour ! Comment puis-je vous aider aujourd'hui ? üòä")
            
            # Stream la r√©ponse caract√®re par caract√®re pour simuler le streaming
            for char in polite_response:
                yield char
            
            # Mettre √† jour l'historique pour la salutation
            self.chat_history.append({
                "user": user_query,
                "assistant": polite_response
            })
            return

        # 2. Recherche dans Qdrant pour les requ√™tes non-salutations
        query_embedding = self.generate_query_embedding(user_query)
        print(f"[DEBUG] Embedding g√©n√©r√© (5 premiers) : {query_embedding[:5]}")
        
        search_results = self.search_qdrant(query_embedding)
        
        relevant_texts = []
        for hit in search_results:
            print(f"[DEBUG] ‚Üí score = {hit.score}")
            if hit.score >= self.score_threshold:
                texte = hit.payload.get('texte_nettoye', '')
                if texte:
                    relevant_texts.append(texte)

        # 3. Si aucun contexte pertinent n'est trouv√©
        if not relevant_texts:
            print("[DEBUG] Aucun contexte pertinent trouv√© dans Qdrant.")
            no_context_response = "D√©sol√©, je n'ai pas trouv√© d'informations pertinentes pour votre question."
            
            for char in no_context_response:
                yield char
                
            # Mettre √† jour l'historique
            self.chat_history.append({
                "user": user_query,
                "assistant": no_context_response
            })
            return

        # 4. Construire le contexte et les messages avec historique
        context = "\\n---\\n".join(relevant_texts)
        messages = self.build_messages_with_history(user_query, context)
        
        print(f"[DEBUG] Messages envoy√©s √† Mistral (nombre de messages) : {len(messages)}")
        print(f"[DEBUG] Dernier message : {messages[-1]['content'][:300]}...")

        # 5. Appel streaming √† l'API Mistral
        headers = {
            "Authorization": f"Bearer {self.mistral_api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.mistral_model,
            "messages": messages,
            "stream": True,
            "temperature": 0.7
        }

        full_response = ""  # Pour stocker la r√©ponse compl√®te
        
        try:
            response = requests.post(self.api_url, headers=headers, json=data, stream=True)
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line and line.startswith(b"data: "):
                    chunk_data = line.decode("utf-8").replace("data: ", "")
                    
                    if chunk_data.strip() == "[DONE]":
                        break
                    
                    try:
                        # Parser le JSON du chunk
                        chunk_json = json.loads(chunk_data)
                        if "choices" in chunk_json and len(chunk_json["choices"]) > 0:
                            delta = chunk_json["choices"][0].get("delta", {})
                            if "content" in delta:
                                content = delta["content"]
                                full_response += content
                                yield content
                    except json.JSONDecodeError:
                        # Ignorer les chunks malform√©s
                        continue
                        
        except Exception as e:
            error_msg = f"[ERREUR STREAMING MISTRAL] {str(e)}"
            print(f"[DEBUG] {error_msg}")
            yield error_msg
            full_response = error_msg

        # 6. Mettre √† jour l'historique avec la r√©ponse compl√®te
        if full_response:
            self.chat_history.append({
                "user": user_query,
                "assistant": full_response
            })

    def call_mistral_api_with_messages(self, messages):
        """Appel non-streaming √† l'API Mistral (conserv√© pour compatibilit√©)"""
        headers = {
            "Authorization": f"Bearer {self.mistral_api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": self.mistral_model,
            "messages": messages,
            "temperature": 0.7
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=data)
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                return f"[Erreur Mistral API] {response.status_code} - {response.text}"
        except Exception as e:
            return f"[Exception Mistral API] {str(e)}"

    def process_query(self, user_query):
        """
        M√©thode h√©rit√©e pour compatibilit√© - utilise maintenant stream_response
        et retourne la r√©ponse compl√®te au lieu de streamer
        """
        print(f"[DEBUG] process_query appel√© - redirection vers stream_response")
        
        # Collecter toute la r√©ponse stream√©e
        full_response = ""
        for chunk in self.stream_response(user_query):
            full_response += chunk
            
        return full_response

    def get_chat_history(self):
        """Retourne l'historique des conversations"""
        return self.chat_history

    def clear_chat_history(self):
        """Efface l'historique des conversations"""
        self.chat_history = []

    def set_chat_history(self, history):
        """D√©finit l'historique des conversations"""
        self.chat_history = history
